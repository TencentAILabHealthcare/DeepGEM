# CONFIG


DATA_TABLE: ""
NUM_FOLD: 100
LABEL_NAME: "EGFR"


PATCH:
    IDX2IMG: './data/internal/dict_name2imgs_path_rmbg.pkl'

PRETRAIN:
    MODEL_NAME: 'ctranspath'
    MODEL_PATH: './checkpoints/pretrain/ctranspath.pth'
    SAVE_DIR: './data/internal/feat'
    SAVE_DIR_COMBINED: './data/internal/combined_feat'
    TRAIN_FEA_TIMES: 1

MODEL:
    MODEL_NAME: 'deepgem'

DATASET:
    DATASET_NAME: 'dataset_deepgem'
    DATASET_SEED: 0
    DATASET_SCALE: 'x20'
    FEATURE_MAP_SIZE: 80
    PATCH_LEVEL: True
    FEATURE_LEN: 500

MODEL_VIT:
    NUM_CLASSES: 2
    DROPOUT: 0.5
    NUM_INPUT_CHANNELS: 768 # feature dim
    HIDDEN_DIM: 64 # Size of the embeddings (dimension of the transformer)
    DEPTH: 3
    HEADS: 4
    MLP_DIM: 64

TRAIN:
    LOSS_NAME: 'be'
    EPOCHS: 300
    START_EPOCH: 0
    BATCH_SIZE: 32
    NUM_WORKERS: 0
    LR: 2e-4
    LR_DROP: 100
    LR_BACKBONE: 2e-5
    LR_LINEAR_PROJ_MULT: 0.1
    WEIGHT_DECAY: 1e-4
    CLIP_MAX_NORM: 0.01
    OPTIM_NAME: 'adamw'
    EVAL: False
    RESUME_PATH: ''
    OUTPUT_DIR: './result/internal'
    SEED: 666
    CACHE_MODE: False
    EMA: 0.99
    WARMUP: 10